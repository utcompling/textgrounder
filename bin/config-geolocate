#!/bin/sh

# This script computes the paths of all files needed by any of the
# preprocessing or experiment-running apps in TextGrounder -- in particular
# in the Geolocate portion.  Note that there are multiple corpora
# (and possibly multiple corpora, e.g. different dumps of Wikipedia at
# different times), and typically multiple files per corpus.

# You need to set the following environment variables on input:

# 1. POLIGROUNDER_DIR: Always required, points to top level of TextGrounder
#                      distribution.
# 2. Variables pointing to the corpora files: Either TG_CORPUS_DIR (which
#    points to where the corpora are stored), or some other variable that
#    triggers the setting of TG_CORPUS_DIR, as follows:
#
#    a. At the lowest level, you can simply set TG_CORPUS_DIR directly,
#       and if set it will be used in preference to any of the methods below.
#
#    b. If you have the data files in a directory structure like the one on
#       the UTexas comp ling machines with the corpora stored under a
#       directory `.../corpora` and other stuff stored in particular places
#       (e.g. auxiliary data, used for the `geolocate-toponym` subproject,
#       stored in `.../projects/textgrounder/data`), you can set
#       TG_GROUPS_DIR to the root of that directory structure.
#
#    c. If you're actually on the UTexas comp ling machines, which put the
#       above directories under `/groups`, or you have exactly the same
#       structure on another machine, just set TG_ON_COMP_LING_MACHINES to
#       "yes", and the variables will all get set properly.
#
#    d. If you're on the UTexas Longhorn cluster, you can use the data
#       stored under `/scratch/01683/benwing` by setting TG_ON_LONGHORN to
#       "yes", and the variables will all get set properly.
#
#    e. If you're running under Hadoop, set TG_USE_HDFS to "yes"; then,
#       TG_GROUPS_DIR will be set to the value of TG_HADOOP_DIR
#       (see below; actually refers to a directory in the Hadoop File System),
#       and the above data-file variables will be set appropriately.  Use
#       the script 'tg-copy-data-to-hadoop' to copy data files to the
#       Hadoop File System underneath TG_HADOOP_DIR.
#
# 3. TG_HADOOP_DIR: Optional; location of corpus and TextGrounder data files
#                   in the Hadoop File System (HDFS).  By default, set to the
#                   relative directory 'textgrounder-data', which means it
#                   is relative to your home directory in HDFS, which is
#                   normally /user/$USER.  See also the variable TG_USE_HDFS,
#                   described above.
#
#
# The following two are used during preprocessing of a Wikipedia dump.
# Normally, it isn't necessary to set these variables, as they will be set
# automatically when using either the highest-level preprocessing script
# `download-preprocess-wiki` or the next-level scripts `preprocess-dump` and
# `convert-corpus-to-latest`.
#
# 4. USE_PERMUTED: Optional; if "no", don't use permuted version of
#                  Wikipedia data files. If "yes", do use it.  Otherwise,
#                  use it if it appears available, otherwise not.
#
# 5. WP_VERSION: Optional; if set, should specify the prefix of a dump file,
#                which will be used for the dump file and all files computed
#                from the dump file.  An example prefix is 'enwiki-20111007',
#                which indicates the English Wikipedia dump of Oct 7, 2011.
#                (This format comes from the naming of dump files on the
#                Wikipedia servers; we didn't make it up.) If not set, will
#                use one of the current dumps that have been completely
#                preprocessed -- currently 'enwiki-20100905', the version
#                used in Wing and Baldridge 2011.

if [ -z "$POLIGROUNDER_DIR" ]; then
  echo "Must set POLIGROUNDER_DIR to top level of TextGrounder distribution"
  exit 1
fi

CONFIG_GEOLOCATE_INCLUDED=yes

TG_PYTHON_DIR="$POLIGROUNDER_DIR/src/main/python"

TG_HADOOP_DIR=${TG_HADOOP_DIR:-textgrounder-data}

if [ "$HADOOP" != yes ]; then
  TG_REAL_USE_HDFS=no
else
  TG_REAL_USE_HDFS=${TG_USE_HDFS:-no}
fi

if [ "$TG_REAL_USE_HDFS" = yes ]; then
  TG_GROUPS_DIR=$TG_HADOOP_DIR
elif [ "$TG_ON_COMP_LING_MACHINES" = yes ]; then
  TG_GROUPS_DIR=/groups
elif [ "$TG_ON_LONGHORN" = yes ]; then
  TG_GROUPS_DIR=/scratch/01683/benwing
fi

if [ -n "$TG_GROUPS_DIR" ]; then
  TG_CORPUS_DIR=${TG_CORPUS_DIR:-$TG_GROUPS_DIR/corpora}
fi

if [ -z "$TG_CORPUS_DIR" ]; then
  echo "Must set TG_CORPUS_DIR to location of TextGrounder corpus files"
  exit 1
fi

if [ "$TG_REAL_USE_HDFS" = yes ]; then
  TG_FILE_PREF=
  POLIGROUNDER_URL=$TG_HADOOP_DIR
elif [ "$HADOOP" = yes ]; then
  TG_FILE_PREF=file:
  POLIGROUNDER_URL=file:$POLIGROUNDER_DIR
else
  TG_FILE_PREF=
  POLIGROUNDER_URL=$POLIGROUNDER_DIR
fi

TG_CORPUS_URL=$TG_FILE_PREF$TG_CORPUS_DIR

GEOTEXT_CORPUS_TOP="$TG_CORPUS_DIR/twitter-geotext/GeoText.2010-10-12"
#GEOTEXT_INPUT_DIR="$GEOTEXT_CORPUS_TOP/processed_data"
GEOTEXT_OUTPUT_DIR="$TG_CORPUS_DIR/twitter-geotext/output-40-docthresh"

# The partial names of the various files we generate or read from -- i.e.
# minus any dump prefix or path.
ORIG_DOCUMENT_DATA_SUFFIX="document-data.txt"
COMBINED_DOCUMENT_DATA_SUFFIX="combined-document-data.txt"
COORDS_SUFFIX="coords.txt"
COORD_COUNTS_SUFFIX="counts-only-coord-documents.txt"
ALL_COUNTS_SUFFIX="counts-all-documents.txt"
COORD_WORDS_SUFFIX="text-only-coord-documents.txt"
COORD_WORDS_UNTOK_SUFFIX="text-untok-only-coord-documents.txt"
ALL_WORDS_SUFFIX="text-all-documents.txt"
ALL_WORDS_UNTOK_SUFFIX="text-untok-all-documents.txt"
COORD_LINKS_SUFFIX="links-only-coord-documents.txt"
ALL_LINKS_SUFFIX="links-all-documents.txt"
TOPONYM_EVAL_SUFFIX="toponym-eval.txt"
DISAMBIG_ID_SUFFIX="disambig.id.txt"
TITLE2ID_SUFFIX="title2id.txt"
DUMP_SUFFIX="xml.bz2"

if [ -z "$WP_VERSION" ]; then
  WP_VERSION="enwiki-20100905"
fi

set_wp_version() {
  WP_VERSION="$1"
  DUMP_PREFIX="$1"
  WP_VERSION_DIR="$TG_CORPUS_DIR/wikipedia/$WP_VERSION"

  permuted_file="$WP_VERSION_DIR/$WP_VERSION-permuted-pages-articles.xml.bz2"
  if [ -z "$USE_PERMUTED" ]; then
    if [ -e "$permuted_file" ]; then
      DUMP_PREFIX="$1-permuted"
    fi
  elif [ "$USE_PERMUTED" = "yes" ]; then
    if [ -e "$permuted_file" ]; then
      :
    else
      echo "WARNING: Permuted file $permuted_file does not appear to exist, but using anyway."
    fi
    DUMP_PREFIX="$1-permuted"
  fi

  set_wp_version_specific_vars
}

set_wp_version_specific_vars() {
  PAGES_ARTICLES_PREFIX="$DUMP_PREFIX-pages-articles"

  OUT_ORIG_DOCUMENT_DATA_FILE="$DUMP_PREFIX-$ORIG_DOCUMENT_DATA_SUFFIX"
  OUT_COMBINED_DOCUMENT_DATA_FILE="$DUMP_PREFIX-$COMBINED_DOCUMENT_DATA_SUFFIX"
  OUT_COORDS_FILE="$DUMP_PREFIX-$COORDS_SUFFIX"
  OUT_COORD_COUNTS_FILE="$DUMP_PREFIX-$COORD_COUNTS_SUFFIX"
  OUT_ALL_COUNTS_FILE="$DUMP_PREFIX-$ALL_COUNTS_SUFFIX"
  OUT_COORD_WORDS_FILE="$DUMP_PREFIX-$COORD_WORDS_SUFFIX"
  OUT_COORD_WORDS_UNTOK_FILE="$DUMP_PREFIX-$COORD_WORDS_UNTOK_SUFFIX"
  OUT_ALL_WORDS_FILE="$DUMP_PREFIX-$ALL_WORDS_SUFFIX"
  OUT_ALL_WORDS_UNTOK_FILE="$DUMP_PREFIX-$ALL_WORDS_UNTOK_SUFFIX"
  OUT_COORD_LINKS_FILE="$DUMP_PREFIX-$COORD_LINKS_SUFFIX"
  OUT_ALL_LINKS_FILE="$DUMP_PREFIX-$ALL_LINKS_SUFFIX"
  OUT_TOPONYM_EVAL_FILE="$DUMP_PREFIX-$TOPONYM_EVAL_SUFFIX"
  OUT_DISAMBIG_ID_FILE="$PAGES_ARTICLES_PREFIX.$DISAMBIG_ID_SUFFIX"
  OUT_TITLE2ID_FILE="$PAGES_ARTICLES_PREFIX.$TITLE2ID_SUFFIX"
  OUT_DUMP_FILE="$PAGES_ARTICLES_PREFIX.$DUMP_SUFFIX"

  #IN_DISAMBIG_ID_FILE="$WP_VERSION_DIR/$OUT_DISAMBIG_ID_FILE"
}

set_wp_version $WP_VERSION

# Include local configuration if it exists

if [ -e "$POLIGROUNDER_DIR/bin/local-config-geolocate" ]; then
  . "$POLIGROUNDER_DIR/bin/local-config-geolocate"
fi

